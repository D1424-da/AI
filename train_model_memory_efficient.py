# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆæ­ç¨®åˆ†é¡è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2
from pathlib import Path
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import gc
import psutil
from datetime import datetime
from config_loader import ConfigLoader

class MemoryOptimizedPileClassifierTrainer:
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆæ­ç¨®åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, data_dir, model_save_path="models/all_pile_classifier.h5", config_path="config.json"):
        self.data_dir = Path(data_dir)
        self.model_save_path = model_save_path
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        self.config = ConfigLoader(config_path)
        
        # è¨­å®šã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
        self.image_size = self.config.image_size
        self.batch_size = self.config.batch_size  
        self.epochs = self.config.epochs
        
        # ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        self.class_names = self.config.class_order
        if not self.class_names:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: 12ã‚¯ãƒ©ã‚¹å®šç¾©
            self.class_names = [
                'plastic', 'plate', 'byou', 'concrete',
                'traverse', 'kokudo', 'gaiku_sankaku', 'gaiku_setsu',
                'gaiku_takaku', 'gaiku_hojo', 'traverse_in', 'kagoshima_in'
            ]
        
        self.label_encoder = LabelEncoder()
        self.model = None
        
        # è¨­å®šæƒ…å ±è¡¨ç¤º
        print(f"ğŸ”§ è¨­å®šæƒ…å ±:")
        print(f"   ã‚¯ãƒ©ã‚¹æ•°: {len(self.class_names)}")
        print(f"   ã‚¯ãƒ©ã‚¹é †åº: {self.class_names}")
        print(f"   ç”»åƒã‚µã‚¤ã‚º: {self.image_size}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {self.epochs}")
        print(f"   å­¦ç¿’ç‡: {self.config.learning_rate}")
        print(f"   ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: {self.config.memory_optimization}")
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
        self.enable_memory_optimization()
    
    def enable_memory_optimization(self):
        """TensorFlowãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š"""
        # ãƒ¡ãƒ¢ãƒªå¢—åŠ ã‚’æ®µéšçš„ã«è¨­å®š
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        
        # CPUä½¿ç”¨æ™‚ã®æœ€é©åŒ–
        tf.config.threading.set_intra_op_parallelism_threads(4)
        tf.config.threading.set_inter_op_parallelism_threads(2)
    
    def check_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯"""
        memory = psutil.virtual_memory()
        usage_percent = memory.percent
        available_gb = memory.available / (1024**3)
        
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {usage_percent:.1f}% (åˆ©ç”¨å¯èƒ½: {available_gb:.1f}GB)")
        
        if usage_percent > 90:
            print("è­¦å‘Š: ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒå±é™ºãƒ¬ãƒ™ãƒ«ã§ã™")
            return False
        return True
    
    def memory_efficient_image_generator(self, image_files, labels, batch_size):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªç”»åƒã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
        while True:
            # ãƒãƒƒãƒã”ã¨ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            indices = np.random.permutation(len(image_files))
            
            for i in range(0, len(indices), batch_size):
                batch_indices = indices[i:i + batch_size]
                batch_images = []
                batch_labels = []
                
                for idx in batch_indices:
                    # ç”»åƒã‚’å³åº§ã«å‡¦ç†ãƒ»æ­£è¦åŒ–
                    image = self.load_and_preprocess_image_efficient(image_files[idx])
                    if image is not None:
                        batch_images.append(image)
                        batch_labels.append(labels[idx])
                
                if batch_images:
                    # NumPyé…åˆ—ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
                    X_batch = np.array(batch_images, dtype=np.float32)
                    y_batch = np.array(batch_labels)
                    
                    # å³åº§ã«ãƒ¡ãƒ¢ãƒªé–‹æ”¾
                    del batch_images
                    gc.collect()
                    
                    yield X_batch, y_batch
    
    def load_and_preprocess_image_efficient(self, image_path):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ã®ç”»åƒèª­ã¿è¾¼ã¿"""
        try:
            # PILã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„
            from PIL import Image
            with Image.open(image_path) as img:
                # RGBå¤‰æ›
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # ãƒªã‚µã‚¤ã‚ºï¼ˆé«˜å“è³ªï¼‰
                img = img.resize(self.image_size, Image.LANCZOS)
                
                # NumPyé…åˆ—ã«å¤‰æ›ãƒ»æ­£è¦åŒ–
                image_array = np.array(img, dtype=np.float32) / 255.0
                
                return image_array
        
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {image_path} - {str(e)}")
            return None
    
    def prepare_file_lists_only(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ã¿ã‚’æº–å‚™ï¼ˆç”»åƒã¯èª­ã¿è¾¼ã¾ãªã„ï¼‰"""
        print("ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆæº–å‚™ä¸­...")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}")
        
        image_files = []
        labels = []
        class_counts = {}
        
        for class_name in self.class_names:
            class_dir = self.data_dir / class_name
            if not class_dir.exists():
                print(f"âš ï¸ è­¦å‘Š: {class_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ã¿åé›†ï¼ˆé‡è¤‡é™¤å»ï¼‰
            class_files = set()  # é‡è¤‡æ’é™¤ã®ãŸã‚setã‚’ä½¿ç”¨
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯
            for file_path in class_dir.iterdir():
                if file_path.is_file():
                    ext = file_path.suffix.lower()
                    if ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                        class_files.add(file_path)
            
            class_files = list(class_files)  # ãƒªã‚¹ãƒˆã«æˆ»ã™
            
            class_counts[class_name] = len(class_files)
            print(f"   {class_name}: {len(class_files):,}æš")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
            image_files.extend(class_files)
            labels.extend([class_name] * len(class_files))
        
        if not image_files:
            raise ValueError("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆé †åºã‚’ä¿æŒï¼‰
        # ğŸš¨ CRITICAL: LabelEncoderã¯è‡ªå‹•ã‚½ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€æ‰‹å‹•ãƒãƒƒãƒ”ãƒ³ã‚°ã§é †åºä¿æŒ
        class_to_index = {class_name: i for i, class_name in enumerate(self.class_names)}
        encoded_labels = [class_to_index[label] for label in labels]
        
        # LabelEncoderã‚’æ­£ã—ã„é †åºã§åˆæœŸåŒ–ï¼ˆæ¨è«–æ™‚ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
        self.label_encoder.classes_ = np.array(self.class_names)
        
        categorical_labels = keras.utils.to_categorical(encoded_labels, len(self.class_names))
        
        print(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(image_files)}")
        print("ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:", class_counts)
        print(f"ğŸ” å®šç¾©ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹é †åº: {self.class_names}")
        print(f"ğŸ” LabelEncoderé †åº: {self.label_encoder.classes_.tolist()}")
        print(f"ğŸ” ã‚¯ãƒ©ã‚¹â†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°: {class_to_index}")
        
        return image_files, categorical_labels
    
    def create_memory_efficient_model(self):
        """320x320é«˜è§£åƒåº¦å¯¾å¿œMobileNetV2ãƒ¢ãƒ‡ãƒ«"""
        print("320x320é«˜è§£åƒåº¦MobileNetV2ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
        
        # MobileNetV2: ãƒ•ãƒ«å¹…ã§320x320å¯¾å¿œ
        base_model = keras.applications.MobileNetV2(
            weights='imagenet',
            include_top=False,
            input_shape=(*self.image_size, 3),
            alpha=1.0  # ãƒ•ãƒ«å¹…ï¼ˆ0.75â†’1.0ã«å¤‰æ›´ï¼‰
        )
        
        # Fine-tuning: ä¸Šä½30å±¤ã‚’è¨“ç·´å¯èƒ½ã«
        base_model.trainable = True
        for layer in base_model.layers[:-30]:
            layer.trainable = False
        
        # æ·±ã„åˆ†é¡ãƒ˜ãƒƒãƒ‰ï¼ˆ224x224ç‰ˆã®2å€ã®å®¹é‡ï¼‰
        model = keras.Sequential([
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            layers.Dense(len(self.class_names), activation='softmax')
        ])
        
        model.compile(
            optimizer=keras.optimizers.AdamW(
                learning_rate=self.config.learning_rate,
                weight_decay=0.0001
            ),
            loss='categorical_crossentropy',
            metrics=['accuracy',]
        )
        
        print(f"320x320ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†:")
        print(f"   å…¥åŠ›: {self.image_size} (224x224ã®2.04å€ã®è§£åƒåº¦)")
        print(f"   alpha: 1.0 (ãƒ•ãƒ«å¹…)")
        print(f"   åˆ†é¡ãƒ˜ãƒƒãƒ‰: 512â†’256 (224x224ç‰ˆã¯128ã®ã¿)")
        print(f"   Fine-tuning: ä¸Šä½30å±¤")
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.count_params():,}")
        
        self.model = model
        return model
    
    def train_with_generator(self, train_files, train_labels, val_files, val_labels):
        """ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ãŸè¨“ç·´"""
        print("ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼è¨“ç·´é–‹å§‹...")
        
        # ã‚¹ãƒ†ãƒƒãƒ—æ•°è¨ˆç®—
        train_steps = len(train_files) // self.batch_size
        val_steps = len(val_files) // self.batch_size
        
        print(f"è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—/ã‚¨ãƒãƒƒã‚¯: {train_steps}")
        print(f"æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—/ã‚¨ãƒãƒƒã‚¯: {val_steps}")
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
        train_gen = self.memory_efficient_image_generator(
            train_files, train_labels, self.batch_size
        )
        val_gen = self.memory_efficient_image_generator(
            val_files, val_labels, self.batch_size
        )
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ãï¼‰
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            ),
            keras.callbacks.ModelCheckpoint(
                self.model_save_path,
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: self.memory_callback(epoch, logs)
            )
        ]
        
        # è¨“ç·´å®Ÿè¡Œ
        history = self.model.fit(
            train_gen,
            steps_per_epoch=train_steps,
            epochs=self.epochs,
            validation_data=val_gen,
            validation_steps=val_steps,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def memory_callback(self, epoch, logs):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        if epoch % 5 == 0:  # 5ã‚¨ãƒãƒƒã‚¯ã”ã¨
            memory = psutil.virtual_memory()
            print(f"ã‚¨ãƒãƒƒã‚¯ {epoch+1} - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
            
            if memory.percent > 95:
                print("è­¦å‘Š: ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒå±é™ºãƒ¬ãƒ™ãƒ«ã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")
                gc.collect()
    
    def train_optimized(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸè¨“ç·´å®Ÿè¡Œ"""
        try:
            print("=== ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨“ç·´é–‹å§‹ ===")
            
            # åˆæœŸãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
            if not self.check_memory_usage():
                print("è­¦å‘Š: æ—¢ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã™ãã¾ã™")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆæº–å‚™ï¼ˆç”»åƒã¯èª­ã¿è¾¼ã¾ãªã„ï¼‰
            image_files, labels = self.prepare_file_lists_only()
            
            # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²
            train_files, val_files, train_labels, val_labels = train_test_split(
                image_files, labels, test_size=0.2, random_state=42,
                stratify=np.argmax(labels, axis=1)
            )
            
            print(f"è¨“ç·´ãƒ•ã‚¡ã‚¤ãƒ«: {len(train_files)}")
            print(f"æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«: {len(val_files)}")
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            self.model = self.create_memory_efficient_model()
            
            # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼è¨“ç·´
            history = self.train_with_generator(
                train_files, train_labels, val_files, val_labels
            )
            
            # çµæœè¡¨ç¤º
            self.plot_results(history)
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜
            self.save_model_info()
            
            print("è¨“ç·´å®Œäº†!")
            return True
            
        except Exception as e:
            print(f"è¨“ç·´ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
        finally:
            # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            gc.collect()
    
    def plot_results(self, history):
        """çµæœå¯è¦–åŒ–"""
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='è¨“ç·´ç²¾åº¦')
        plt.plot(history.history['val_accuracy'], label='æ¤œè¨¼ç²¾åº¦')
        plt.title('ç²¾åº¦æ¨ç§»')
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='è¨“ç·´æå¤±')
        plt.plot(history.history['val_loss'], label='æ¤œè¨¼æå¤±')
        plt.title('æå¤±æ¨ç§»')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('training_results.png')
        plt.show()
    
    def save_model_info(self):
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜ï¼ˆè©³ç´°ç‰ˆï¼‰"""
        model_info = {
            'model_type': 'all_pile_classifier',
            'class_names': self.class_names,  # å®šç¾©é †åº
            'image_size': list(self.image_size),
            'batch_size': self.batch_size,
            'epochs': self.epochs,
            'learning_rate': self.config.learning_rate,
            'target_accuracy': 0.90,
            'model_save_path': self.model_save_path,
            'memory_optimized': True,
            'total_classes': len(self.class_names),
            'label_encoder_classes': self.label_encoder.classes_.tolist(),  # å®Ÿéš›ã®é †åº
            'training_timestamp': datetime.now().isoformat()
        }
        
        # ğŸš¨ é †åºæ•´åˆæ€§ã®æ¤œè¨¼
        print(f"ğŸ” ã‚¯ãƒ©ã‚¹é †åºæ¤œè¨¼:")
        print(f"   å®šç¾©é †åº: {self.class_names}")
        print(f"   Encoderé †åº: {self.label_encoder.classes_.tolist()}")
        
        if self.class_names != self.label_encoder.classes_.tolist():
            print("âš ï¸  è­¦å‘Š: ã‚¯ãƒ©ã‚¹é †åºãŒä¸ä¸€è‡´ï¼")
            print("   æ¨è«–æ™‚ã«èª¤åˆ†é¡ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        else:
            print("âœ… ã‚¯ãƒ©ã‚¹é †åºä¸€è‡´ç¢ºèª")
        
        # ãƒ¡ã‚¤ãƒ³ã® model_info.json ã‚’ä¿å­˜
        info_path = Path(self.model_save_path).parent / "model_info.json"
        os.makedirs(Path(self.model_save_path).parent, exist_ok=True)
        
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        # 12ã‚¯ãƒ©ã‚¹å°‚ç”¨ã®æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
        all_pile_info_path = Path(self.model_save_path).parent / "all_pile_model_info.json"
        with open(all_pile_info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«æƒ…å ±ä¿å­˜å®Œäº†:")
        print(f"   {info_path}")
        print(f"   {all_pile_info_path}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¸Šä½éšå±¤ã«è¨­å®š
    trainer = MemoryOptimizedPileClassifierTrainer('../training_data')
    trainer.train_optimized()
    