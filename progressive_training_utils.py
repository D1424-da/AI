"""
æ®µéšçš„å­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ - 13ä¸‡æšå¯¾å¿œ
Progressive Training Utilities for Ultra-Large Datasets
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import gc
import psutil
from pathlib import Path


class ProgressiveStageGenerator(keras.utils.Sequence):
    """æ®µéšçš„å­¦ç¿’ç”¨ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
    
    def __init__(self, base_generator, subset_size, batch_size, stage_name="stage"):
        self.base_generator = base_generator
        self.subset_size = min(subset_size, len(base_generator.filepaths))
        self.batch_size = batch_size
        self.stage_name = stage_name
        
        # ã‚µãƒ–ã‚»ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é¸æŠï¼ˆãƒãƒ©ãƒ³ã‚¹è€ƒæ…®ï¼‰
        self.indices = self._select_balanced_subset()
        
        print(f"   ğŸ“Š {stage_name}: {self.subset_size:,}æšé¸æŠ (batch_size={batch_size})")
        
    def _select_balanced_subset(self):
        """ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸã‚µãƒ–ã‚»ãƒƒãƒˆé¸æŠ"""
        try:
            # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«é¸æŠ
            all_labels = np.argmax(self.base_generator.labels, axis=1)
            unique_classes = np.unique(all_labels)
            samples_per_class = self.subset_size // len(unique_classes)
            
            selected_indices = []
            
            for class_id in unique_classes:
                class_indices = np.where(all_labels == class_id)[0]
                
                if len(class_indices) > samples_per_class:
                    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    selected = np.random.choice(
                        class_indices, 
                        samples_per_class, 
                        replace=False
                    )
                else:
                    # å…¨ã¦é¸æŠ
                    selected = class_indices
                    
                selected_indices.extend(selected)
            
            # ä¸è¶³åˆ†ã‚’è¿½åŠ 
            remaining = self.subset_size - len(selected_indices)
            if remaining > 0:
                all_indices = set(range(len(self.base_generator.filepaths)))
                unused_indices = list(all_indices - set(selected_indices))
                
                if unused_indices:
                    additional = np.random.choice(
                        unused_indices,
                        min(remaining, len(unused_indices)),
                        replace=False
                    )
                    selected_indices.extend(additional)
            
            return np.array(selected_indices[:self.subset_size])
            
        except Exception as e:
            print(f"   âš ï¸  ãƒãƒ©ãƒ³ã‚¹é¸æŠå¤±æ•—ã€ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ: {e}")
            return np.random.choice(
                len(self.base_generator.filepaths),
                self.subset_size,
                replace=False
            )
    
    def __len__(self):
        return int(np.ceil(self.subset_size / self.batch_size))
    
    def __getitem__(self, idx):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—"""
        batch_start = idx * self.batch_size
        batch_end = min(batch_start + self.batch_size, self.subset_size)
        batch_indices = self.indices[batch_start:batch_end]
        
        # ãƒ™ãƒ¼ã‚¹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        images = []
        labels = []
        
        for idx in batch_indices:
            try:
                # ãƒ™ãƒ¼ã‚¹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®ç”»åƒãƒ»ãƒ©ãƒ™ãƒ«å–å¾—
                img_path = self.base_generator.filepaths[idx]
                label = self.base_generator.labels[idx]
                
                # ç”»åƒèª­ã¿è¾¼ã¿ï¼ˆPILä½¿ç”¨ï¼‰
                img = self.base_generator.load_and_preprocess_image(img_path)
                
                if img is not None:
                    images.append(img)
                    labels.append(label)
                    
            except Exception as e:
                print(f"   âš ï¸  ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not images:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿è¿”å´
            return (
                np.zeros((1, *self.base_generator.image_size, 3)),
                np.zeros((1, len(self.base_generator.class_names)))
            )
        
        return np.array(images), np.array(labels)
    
    def on_epoch_end(self):
        """ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®å‡¦ç†"""
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if hasattr(self, '_cleanup_counter'):
            self._cleanup_counter += 1
        else:
            self._cleanup_counter = 1
            
        if self._cleanup_counter % 3 == 0:  # 3ã‚¨ãƒãƒƒã‚¯ã”ã¨
            gc.collect()
            
            memory = psutil.virtual_memory()
            if memory.percent > 85:
                print(f"   ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {memory.percent:.1f}% â†’ ", end="")
                gc.collect()
                tf.keras.backend.clear_session()
                memory_after = psutil.virtual_memory()
                print(f"{memory_after.percent:.1f}%")


class UltraLargeDatasetTrainer:
    """13ä¸‡æšå¯¾å¿œã®æ®µéšçš„å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(self, base_trainer):
        self.base_trainer = base_trainer
        self.progressive_stages = None
        
    def setup_progressive_stages(self, total_images):
        """æ®µéšçš„å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®š"""
        if total_images > 100000:
            # 13ä¸‡æšå¯¾å¿œã®3æ®µéšå­¦ç¿’
            self.progressive_stages = {
                'foundation': {
                    'subset': 25000,
                    'epochs': 15,
                    'lr': 0.001,
                    'batch': 64,
                    'description': 'åŸºç¤å­¦ç¿’ï¼ˆ2.5ä¸‡æšï¼‰'
                },
                'expansion': {
                    'subset': 70000,
                    'epochs': 12,
                    'lr': 0.0003,
                    'batch': 96,
                    'description': 'æ‹¡å¼µå­¦ç¿’ï¼ˆ7ä¸‡æšï¼‰'
                },
                'refinement': {
                    'subset': total_images,
                    'epochs': 10,
                    'lr': 0.00005,
                    'batch': 128,
                    'description': f'é«˜ç²¾åº¦å­¦ç¿’ï¼ˆ{total_images:,}æšï¼‰'
                }
            }
        elif total_images > 50000:
            # 5-10ä¸‡æšç”¨ã®2æ®µéšå­¦ç¿’
            self.progressive_stages = {
                'foundation': {
                    'subset': total_images // 2,
                    'epochs': 20,
                    'lr': 0.001,
                    'batch': 64,
                    'description': f'åŸºç¤å­¦ç¿’ï¼ˆ{total_images//2:,}æšï¼‰'
                },
                'refinement': {
                    'subset': total_images,
                    'epochs': 15,
                    'lr': 0.0001,
                    'batch': 96,
                    'description': f'é«˜ç²¾åº¦å­¦ç¿’ï¼ˆ{total_images:,}æšï¼‰'
                }
            }
        else:
            # é€šå¸¸å­¦ç¿’
            return False
            
        print(f"\nğŸ¯ æ®µéšçš„å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¸è¨­å®šå®Œäº†:")
        for stage_name, params in self.progressive_stages.items():
            print(f"   {stage_name}: {params['description']}")
            
        return True
    
    def execute_progressive_training(self, train_generator, val_generator):
        """æ®µéšçš„å­¦ç¿’å®Ÿè¡Œ"""
        if not self.progressive_stages:
            return None
            
        print("\nğŸš€ æ®µéšçš„å­¦ç¿’é–‹å§‹")
        all_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        
        for stage_name, params in self.progressive_stages.items():
            print(f"\nğŸ“ˆ {stage_name.upper()}: {params['description']}")
            
            # æ®µéšåˆ¥ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½œæˆ
            stage_generator = ProgressiveStageGenerator(
                train_generator,
                params['subset'],
                params['batch'],
                stage_name
            )
            
            # å­¦ç¿’ç‡èª¿æ•´
            if hasattr(self.base_trainer.model.optimizer, 'learning_rate'):
                self.base_trainer.model.optimizer.learning_rate.assign(params['lr'])
                print(f"   ğŸ“Š å­¦ç¿’ç‡è¨­å®š: {params['lr']}")
            
            # æ®µéšå­¦ç¿’å®Ÿè¡Œ
            stage_history = self.base_trainer.model.fit(
                stage_generator,
                validation_data=val_generator,
                epochs=params['epochs'],
                callbacks=self.base_trainer.get_callbacks(),
                verbose=1
            )
            
            # å±¥æ­´çµ±åˆ
            for metric in all_history.keys():
                if metric in stage_history.history:
                    all_history[metric].extend(stage_history.history[metric])
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸çµæœè¡¨ç¤º
            final_acc = stage_history.history.get('val_accuracy', [0])[-1]
            print(f"   âœ… {stage_name}å®Œäº†: val_accuracy={final_acc:.4f}")
            
            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            gc.collect()
            tf.keras.backend.clear_session()
        
        # ç–‘ä¼¼historyã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        return type('History', (), {'history': all_history})()
    
    def validate_ultra_dataset(self, total_images):
        """è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼"""
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        print(f"\nğŸ” è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼:")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {total_images:,}æš")
        print(f"   åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_gb:.1f}GB")
        
        # ãƒ¡ãƒ¢ãƒªè¦ä»¶æ¨å®š
        estimated_memory_gb = (total_images * 640 * 640 * 3 * 4) / (1024**3)  # float32
        
        if estimated_memory_gb > available_gb * 0.8:
            print(f"   âš ï¸  æ¨å®šãƒ¡ãƒ¢ãƒªè¦ä»¶: {estimated_memory_gb:.1f}GB")
            print(f"   ğŸ¯ æ®µéšçš„å­¦ç¿’ãŒå¿…è¦ã§ã™")
            return True
        else:
            print(f"   âœ… é€šå¸¸å­¦ç¿’ã§å‡¦ç†å¯èƒ½")
            return False